{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 70)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "from bayes_opt import BayesianOptimization #From https://github.com/fmfn/BayesianOptimization\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('sales.csv', nrows=100)\n",
    "\n",
    "float_cols = [c for c in sales if (sales[c].dtype == \"float64\")&('revenue' not in c)&('ratio' not in c)]\n",
    "float16_cols = {c: np.float16 for c in float_cols}\n",
    "\n",
    "int_cols = [c for c in sales if (sales[c].dtype == \"int64\")&(c != 'itemid')]\n",
    "int8_cols = {c: np.int8 for c in int_cols}\n",
    "\n",
    "float16_cols.update(int8_cols)\n",
    "sales = pd.read_csv('sales.csv', engine='c', dtype=float16_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All features were selected using (very time consuming) feature selection, consisting in a mix of intuition,\n",
    "#manual recursive feature elimination, use of feature importance from lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'month',\n",
    "    'shopid',\n",
    "    'itemcategoryid',\n",
    "    'meanlagitemshop',\n",
    "    'meanlagitem',\n",
    "    'cumulativemeanitemshop',\n",
    "    'shopcityid',\n",
    "    'shoptypeid',\n",
    "    'itemcategorytypeid',\n",
    "    'itemcategorysubtypeid',\n",
    "    'nbdays',\n",
    "    'year',\n",
    "    'monthofyear',\n",
    "    'nbweekends',\n",
    "    'ratiolastanteprice',\n",
    "    'ratiolastavgprice',\n",
    "    'ratiolastminprice',\n",
    "    'ratiolastmaxprice',\n",
    "    'ratiorevenue12',\n",
    "    'newitem',\n",
    "    'tsshopid1',\n",
    "    'tsshopid2',\n",
    "    'tsshopid3',\n",
    "    'tsshopid6',\n",
    "    'tsshopid12',\n",
    "    'tsshopcityid1',\n",
    "    'tsshoptypeid1',\n",
    "    'tsitemcategoryid1',\n",
    "    'tsitemcategorysubtypeid1',\n",
    "    'tsitemcategoryiditemcategorytypeid1',\n",
    "    'tsitemcategoryiditemcategorysubtypeid1',\n",
    "    'tsitemcategoryidshopcityid1',\n",
    "    'tsitemcategoryidshoptypeid1',\n",
    "    'tsitemcategoryidshopid1',\n",
    "    'tsitemcategorytypeiditemcategorysubtypeid1',\n",
    "    'tsitemcategorytypeidshopcityid1',\n",
    "    'tsitemcategorytypeidshoptypeid1',\n",
    "    'tsitemcategorytypeidshopid1',\n",
    "    'tsitemcategorysubtypeidshopcityid1',\n",
    "    'tsitemcategorysubtypeidshoptypeid1',\n",
    "    'tsitemcategorysubtypeidshopid1',\n",
    "    'tsshopcityidshoptypeid1',\n",
    "    'tsshopcityidshopid1',\n",
    "    'tsshoptypeidshopid1',\n",
    "    'tsitemid1',\n",
    "    'tsitemid2',\n",
    "    'tsitemid3',\n",
    "    'tsitemid6',\n",
    "    'tsitemid12',\n",
    "    'tsshopiditemid1',\n",
    "    'tsshopiditemid2',\n",
    "    'tsshopiditemid3',\n",
    "    'tsshopiditemid6',\n",
    "    'tsshopiditemid12'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'shopid',\n",
    "    'itemcategoryid',\n",
    "    'shopcityid',\n",
    "    'shoptypeid',\n",
    "    'itemcategorytypeid',\n",
    "    'itemcategorysubtypeid',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train until month 32, predict month 33\n",
    "def run_lgb(features,categorical_features,params):\n",
    "    minmonth = 12\n",
    "    monthpred = 33\n",
    "\n",
    "    ROUNDS = 10000\n",
    "    \n",
    "    traindf = sales[(sales.month < monthpred) & (sales.month >= minmonth)]\n",
    "    testdf = sales[(sales.month == monthpred) & (sales.month >= minmonth)]\n",
    "    \n",
    "    traingbm = lgb.Dataset(traindf[features], label=traindf['target'], categorical_feature=categorical_features, free_raw_data=False)\n",
    "    testgbm = lgb.Dataset(testdf[features], label=testdf['target'], categorical_feature=categorical_features, free_raw_data=False)\n",
    "    \n",
    "    m = lgb.train(params, traingbm, ROUNDS, valid_sets=[traingbm, testgbm], verbose_eval=1)\n",
    "    predsgbm = m.predict(testdf[features])\n",
    "    mse = mean_squared_error(testdf['target'],predsgbm)\n",
    "    rmse = math.sqrt(mse)\n",
    "    print('total rmse : {}'.format(rmse))\n",
    "    return -rmse #(score for Bayesian optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's rmse: 1.16135\tvalid_1's rmse: 1.13081\n",
      "Training until validation scores don't improve for 60 rounds.\n",
      "[2]\ttraining's rmse: 1.13523\tvalid_1's rmse: 1.10946\n",
      "[3]\ttraining's rmse: 1.111\tvalid_1's rmse: 1.08872\n",
      "[4]\ttraining's rmse: 1.08863\tvalid_1's rmse: 1.07032\n",
      "[5]\ttraining's rmse: 1.06794\tvalid_1's rmse: 1.05301\n",
      "[6]\ttraining's rmse: 1.04889\tvalid_1's rmse: 1.0365\n",
      "[7]\ttraining's rmse: 1.03134\tvalid_1's rmse: 1.02208\n",
      "[8]\ttraining's rmse: 1.01499\tvalid_1's rmse: 1.00796\n",
      "[9]\ttraining's rmse: 0.999741\tvalid_1's rmse: 0.994976\n",
      "[10]\ttraining's rmse: 0.985839\tvalid_1's rmse: 0.983602\n",
      "[11]\ttraining's rmse: 0.972997\tvalid_1's rmse: 0.973404\n",
      "[12]\ttraining's rmse: 0.961021\tvalid_1's rmse: 0.963455\n",
      "[13]\ttraining's rmse: 0.950062\tvalid_1's rmse: 0.954546\n",
      "[14]\ttraining's rmse: 0.939822\tvalid_1's rmse: 0.946114\n",
      "[15]\ttraining's rmse: 0.930542\tvalid_1's rmse: 0.939198\n",
      "[16]\ttraining's rmse: 0.921945\tvalid_1's rmse: 0.931859\n",
      "[17]\ttraining's rmse: 0.913756\tvalid_1's rmse: 0.924777\n",
      "[18]\ttraining's rmse: 0.906495\tvalid_1's rmse: 0.918857\n",
      "[19]\ttraining's rmse: 0.899461\tvalid_1's rmse: 0.91275\n",
      "[20]\ttraining's rmse: 0.893164\tvalid_1's rmse: 0.907644\n",
      "[21]\ttraining's rmse: 0.887517\tvalid_1's rmse: 0.903013\n",
      "[22]\ttraining's rmse: 0.882203\tvalid_1's rmse: 0.898898\n",
      "[23]\ttraining's rmse: 0.876968\tvalid_1's rmse: 0.894776\n",
      "[24]\ttraining's rmse: 0.87231\tvalid_1's rmse: 0.891388\n",
      "[25]\ttraining's rmse: 0.867875\tvalid_1's rmse: 0.887851\n",
      "[26]\ttraining's rmse: 0.863995\tvalid_1's rmse: 0.884754\n",
      "[27]\ttraining's rmse: 0.860307\tvalid_1's rmse: 0.881995\n",
      "[28]\ttraining's rmse: 0.857008\tvalid_1's rmse: 0.879088\n",
      "[29]\ttraining's rmse: 0.853689\tvalid_1's rmse: 0.876429\n",
      "[30]\ttraining's rmse: 0.850782\tvalid_1's rmse: 0.874663\n",
      "[31]\ttraining's rmse: 0.84794\tvalid_1's rmse: 0.872389\n",
      "[32]\ttraining's rmse: 0.845261\tvalid_1's rmse: 0.870291\n",
      "[33]\ttraining's rmse: 0.84276\tvalid_1's rmse: 0.868481\n",
      "[34]\ttraining's rmse: 0.84057\tvalid_1's rmse: 0.866996\n",
      "[35]\ttraining's rmse: 0.838335\tvalid_1's rmse: 0.865254\n",
      "[36]\ttraining's rmse: 0.836275\tvalid_1's rmse: 0.864057\n",
      "[37]\ttraining's rmse: 0.834385\tvalid_1's rmse: 0.862678\n",
      "[38]\ttraining's rmse: 0.832491\tvalid_1's rmse: 0.861514\n",
      "[39]\ttraining's rmse: 0.83072\tvalid_1's rmse: 0.860596\n",
      "[40]\ttraining's rmse: 0.828971\tvalid_1's rmse: 0.85942\n",
      "[41]\ttraining's rmse: 0.827546\tvalid_1's rmse: 0.858666\n",
      "[42]\ttraining's rmse: 0.826053\tvalid_1's rmse: 0.857711\n",
      "[43]\ttraining's rmse: 0.824628\tvalid_1's rmse: 0.856532\n",
      "[44]\ttraining's rmse: 0.823308\tvalid_1's rmse: 0.855572\n",
      "[45]\ttraining's rmse: 0.822124\tvalid_1's rmse: 0.854684\n",
      "[46]\ttraining's rmse: 0.82087\tvalid_1's rmse: 0.85359\n",
      "[47]\ttraining's rmse: 0.819841\tvalid_1's rmse: 0.853023\n",
      "[48]\ttraining's rmse: 0.818689\tvalid_1's rmse: 0.852381\n",
      "[49]\ttraining's rmse: 0.817754\tvalid_1's rmse: 0.85189\n",
      "[50]\ttraining's rmse: 0.816792\tvalid_1's rmse: 0.851432\n",
      "[51]\ttraining's rmse: 0.815974\tvalid_1's rmse: 0.850703\n",
      "[52]\ttraining's rmse: 0.815169\tvalid_1's rmse: 0.850429\n",
      "[53]\ttraining's rmse: 0.814397\tvalid_1's rmse: 0.85022\n",
      "[54]\ttraining's rmse: 0.813638\tvalid_1's rmse: 0.8499\n",
      "[55]\ttraining's rmse: 0.812954\tvalid_1's rmse: 0.849578\n",
      "[56]\ttraining's rmse: 0.812323\tvalid_1's rmse: 0.849441\n",
      "[57]\ttraining's rmse: 0.811685\tvalid_1's rmse: 0.849106\n",
      "[58]\ttraining's rmse: 0.810969\tvalid_1's rmse: 0.848917\n",
      "[59]\ttraining's rmse: 0.810318\tvalid_1's rmse: 0.84888\n",
      "[60]\ttraining's rmse: 0.809593\tvalid_1's rmse: 0.848768\n",
      "[61]\ttraining's rmse: 0.809064\tvalid_1's rmse: 0.848718\n",
      "[62]\ttraining's rmse: 0.808466\tvalid_1's rmse: 0.848307\n",
      "[63]\ttraining's rmse: 0.807907\tvalid_1's rmse: 0.848138\n",
      "[64]\ttraining's rmse: 0.807348\tvalid_1's rmse: 0.847913\n",
      "[65]\ttraining's rmse: 0.80678\tvalid_1's rmse: 0.847732\n",
      "[66]\ttraining's rmse: 0.805903\tvalid_1's rmse: 0.847535\n",
      "[67]\ttraining's rmse: 0.805327\tvalid_1's rmse: 0.847188\n",
      "[68]\ttraining's rmse: 0.804781\tvalid_1's rmse: 0.847094\n",
      "[69]\ttraining's rmse: 0.804349\tvalid_1's rmse: 0.847133\n",
      "[70]\ttraining's rmse: 0.803922\tvalid_1's rmse: 0.847017\n",
      "[71]\ttraining's rmse: 0.803386\tvalid_1's rmse: 0.846987\n",
      "[72]\ttraining's rmse: 0.802987\tvalid_1's rmse: 0.846822\n",
      "[73]\ttraining's rmse: 0.802519\tvalid_1's rmse: 0.846768\n",
      "[74]\ttraining's rmse: 0.801823\tvalid_1's rmse: 0.846685\n",
      "[75]\ttraining's rmse: 0.801288\tvalid_1's rmse: 0.846532\n",
      "[76]\ttraining's rmse: 0.800901\tvalid_1's rmse: 0.846458\n",
      "[77]\ttraining's rmse: 0.80054\tvalid_1's rmse: 0.846337\n",
      "[78]\ttraining's rmse: 0.800178\tvalid_1's rmse: 0.846271\n",
      "[79]\ttraining's rmse: 0.79981\tvalid_1's rmse: 0.846471\n",
      "[80]\ttraining's rmse: 0.799403\tvalid_1's rmse: 0.846562\n",
      "[81]\ttraining's rmse: 0.798973\tvalid_1's rmse: 0.845949\n",
      "[82]\ttraining's rmse: 0.798585\tvalid_1's rmse: 0.845907\n",
      "[83]\ttraining's rmse: 0.798313\tvalid_1's rmse: 0.845891\n",
      "[84]\ttraining's rmse: 0.798048\tvalid_1's rmse: 0.845809\n",
      "[85]\ttraining's rmse: 0.797765\tvalid_1's rmse: 0.845875\n",
      "[86]\ttraining's rmse: 0.79749\tvalid_1's rmse: 0.845871\n",
      "[87]\ttraining's rmse: 0.797298\tvalid_1's rmse: 0.845891\n",
      "[88]\ttraining's rmse: 0.796942\tvalid_1's rmse: 0.845989\n",
      "[89]\ttraining's rmse: 0.796577\tvalid_1's rmse: 0.846121\n",
      "[90]\ttraining's rmse: 0.79627\tvalid_1's rmse: 0.846091\n",
      "[91]\ttraining's rmse: 0.796003\tvalid_1's rmse: 0.846113\n",
      "[92]\ttraining's rmse: 0.795725\tvalid_1's rmse: 0.846209\n",
      "[93]\ttraining's rmse: 0.795269\tvalid_1's rmse: 0.846071\n",
      "[94]\ttraining's rmse: 0.794895\tvalid_1's rmse: 0.845731\n",
      "[95]\ttraining's rmse: 0.794522\tvalid_1's rmse: 0.845613\n",
      "[96]\ttraining's rmse: 0.794199\tvalid_1's rmse: 0.845304\n",
      "[97]\ttraining's rmse: 0.794066\tvalid_1's rmse: 0.845254\n",
      "[98]\ttraining's rmse: 0.793827\tvalid_1's rmse: 0.845243\n",
      "[99]\ttraining's rmse: 0.793598\tvalid_1's rmse: 0.845289\n",
      "[100]\ttraining's rmse: 0.793317\tvalid_1's rmse: 0.845334\n",
      "[101]\ttraining's rmse: 0.793057\tvalid_1's rmse: 0.84538\n",
      "[102]\ttraining's rmse: 0.792619\tvalid_1's rmse: 0.845388\n",
      "[103]\ttraining's rmse: 0.79219\tvalid_1's rmse: 0.84559\n",
      "[104]\ttraining's rmse: 0.792006\tvalid_1's rmse: 0.845606\n",
      "[105]\ttraining's rmse: 0.791675\tvalid_1's rmse: 0.845514\n",
      "[106]\ttraining's rmse: 0.791417\tvalid_1's rmse: 0.845535\n",
      "[107]\ttraining's rmse: 0.791202\tvalid_1's rmse: 0.845578\n",
      "[108]\ttraining's rmse: 0.790881\tvalid_1's rmse: 0.845596\n",
      "[109]\ttraining's rmse: 0.790736\tvalid_1's rmse: 0.84553\n",
      "[110]\ttraining's rmse: 0.790393\tvalid_1's rmse: 0.845412\n",
      "[111]\ttraining's rmse: 0.790074\tvalid_1's rmse: 0.845297\n",
      "[112]\ttraining's rmse: 0.789739\tvalid_1's rmse: 0.844929\n",
      "[113]\ttraining's rmse: 0.789609\tvalid_1's rmse: 0.8449\n",
      "[114]\ttraining's rmse: 0.789423\tvalid_1's rmse: 0.844941\n",
      "[115]\ttraining's rmse: 0.789191\tvalid_1's rmse: 0.844901\n",
      "[116]\ttraining's rmse: 0.788903\tvalid_1's rmse: 0.844806\n",
      "[117]\ttraining's rmse: 0.788757\tvalid_1's rmse: 0.844668\n",
      "[118]\ttraining's rmse: 0.78844\tvalid_1's rmse: 0.844689\n",
      "[119]\ttraining's rmse: 0.78825\tvalid_1's rmse: 0.844736\n",
      "[120]\ttraining's rmse: 0.787882\tvalid_1's rmse: 0.844646\n",
      "[121]\ttraining's rmse: 0.787596\tvalid_1's rmse: 0.844608\n",
      "[122]\ttraining's rmse: 0.787445\tvalid_1's rmse: 0.844573\n",
      "[123]\ttraining's rmse: 0.78696\tvalid_1's rmse: 0.844642\n",
      "[124]\ttraining's rmse: 0.786645\tvalid_1's rmse: 0.844506\n",
      "[125]\ttraining's rmse: 0.786336\tvalid_1's rmse: 0.844825\n",
      "[126]\ttraining's rmse: 0.786062\tvalid_1's rmse: 0.844749\n",
      "[127]\ttraining's rmse: 0.785858\tvalid_1's rmse: 0.84463\n",
      "[128]\ttraining's rmse: 0.785674\tvalid_1's rmse: 0.844534\n",
      "[129]\ttraining's rmse: 0.785346\tvalid_1's rmse: 0.844294\n",
      "[130]\ttraining's rmse: 0.784942\tvalid_1's rmse: 0.844183\n",
      "[131]\ttraining's rmse: 0.784752\tvalid_1's rmse: 0.844162\n",
      "[132]\ttraining's rmse: 0.784379\tvalid_1's rmse: 0.843998\n",
      "[133]\ttraining's rmse: 0.784152\tvalid_1's rmse: 0.843984\n",
      "[134]\ttraining's rmse: 0.783923\tvalid_1's rmse: 0.844024\n",
      "[135]\ttraining's rmse: 0.78335\tvalid_1's rmse: 0.844058\n",
      "[136]\ttraining's rmse: 0.783208\tvalid_1's rmse: 0.844088\n",
      "[137]\ttraining's rmse: 0.782984\tvalid_1's rmse: 0.844032\n",
      "[138]\ttraining's rmse: 0.782567\tvalid_1's rmse: 0.844038\n",
      "[139]\ttraining's rmse: 0.782404\tvalid_1's rmse: 0.844012\n",
      "[140]\ttraining's rmse: 0.78232\tvalid_1's rmse: 0.843991\n",
      "[141]\ttraining's rmse: 0.78219\tvalid_1's rmse: 0.843953\n",
      "[142]\ttraining's rmse: 0.781975\tvalid_1's rmse: 0.844036\n",
      "[143]\ttraining's rmse: 0.781797\tvalid_1's rmse: 0.84415\n",
      "[144]\ttraining's rmse: 0.781396\tvalid_1's rmse: 0.844103\n",
      "[145]\ttraining's rmse: 0.781082\tvalid_1's rmse: 0.844053\n",
      "[146]\ttraining's rmse: 0.780789\tvalid_1's rmse: 0.844012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[147]\ttraining's rmse: 0.780524\tvalid_1's rmse: 0.843935\n",
      "[148]\ttraining's rmse: 0.780396\tvalid_1's rmse: 0.843956\n",
      "[149]\ttraining's rmse: 0.7803\tvalid_1's rmse: 0.844023\n",
      "[150]\ttraining's rmse: 0.780145\tvalid_1's rmse: 0.844089\n",
      "[151]\ttraining's rmse: 0.779812\tvalid_1's rmse: 0.84404\n",
      "[152]\ttraining's rmse: 0.7795\tvalid_1's rmse: 0.844075\n",
      "[153]\ttraining's rmse: 0.779254\tvalid_1's rmse: 0.844035\n",
      "[154]\ttraining's rmse: 0.779033\tvalid_1's rmse: 0.844015\n",
      "[155]\ttraining's rmse: 0.778835\tvalid_1's rmse: 0.843959\n",
      "[156]\ttraining's rmse: 0.778698\tvalid_1's rmse: 0.843959\n",
      "[157]\ttraining's rmse: 0.778591\tvalid_1's rmse: 0.843945\n",
      "[158]\ttraining's rmse: 0.778459\tvalid_1's rmse: 0.843899\n",
      "[159]\ttraining's rmse: 0.778241\tvalid_1's rmse: 0.843818\n",
      "[160]\ttraining's rmse: 0.777964\tvalid_1's rmse: 0.843802\n",
      "[161]\ttraining's rmse: 0.77785\tvalid_1's rmse: 0.843789\n",
      "[162]\ttraining's rmse: 0.777613\tvalid_1's rmse: 0.84375\n",
      "[163]\ttraining's rmse: 0.77751\tvalid_1's rmse: 0.843768\n",
      "[164]\ttraining's rmse: 0.777428\tvalid_1's rmse: 0.84381\n",
      "[165]\ttraining's rmse: 0.777273\tvalid_1's rmse: 0.843963\n",
      "[166]\ttraining's rmse: 0.777174\tvalid_1's rmse: 0.84397\n",
      "[167]\ttraining's rmse: 0.777056\tvalid_1's rmse: 0.843947\n",
      "[168]\ttraining's rmse: 0.776905\tvalid_1's rmse: 0.843955\n",
      "[169]\ttraining's rmse: 0.776739\tvalid_1's rmse: 0.843833\n",
      "[170]\ttraining's rmse: 0.77646\tvalid_1's rmse: 0.843833\n",
      "[171]\ttraining's rmse: 0.776337\tvalid_1's rmse: 0.843859\n",
      "[172]\ttraining's rmse: 0.77621\tvalid_1's rmse: 0.843937\n",
      "[173]\ttraining's rmse: 0.776051\tvalid_1's rmse: 0.843961\n",
      "[174]\ttraining's rmse: 0.775921\tvalid_1's rmse: 0.843869\n",
      "[175]\ttraining's rmse: 0.775685\tvalid_1's rmse: 0.844046\n",
      "[176]\ttraining's rmse: 0.775535\tvalid_1's rmse: 0.844025\n",
      "[177]\ttraining's rmse: 0.775407\tvalid_1's rmse: 0.844032\n",
      "[178]\ttraining's rmse: 0.775334\tvalid_1's rmse: 0.844035\n",
      "[179]\ttraining's rmse: 0.775225\tvalid_1's rmse: 0.843994\n",
      "[180]\ttraining's rmse: 0.775158\tvalid_1's rmse: 0.844048\n",
      "[181]\ttraining's rmse: 0.774907\tvalid_1's rmse: 0.844014\n",
      "[182]\ttraining's rmse: 0.774754\tvalid_1's rmse: 0.843956\n",
      "[183]\ttraining's rmse: 0.774519\tvalid_1's rmse: 0.843945\n",
      "[184]\ttraining's rmse: 0.774437\tvalid_1's rmse: 0.843996\n",
      "[185]\ttraining's rmse: 0.774339\tvalid_1's rmse: 0.843988\n",
      "[186]\ttraining's rmse: 0.774257\tvalid_1's rmse: 0.844063\n",
      "[187]\ttraining's rmse: 0.773996\tvalid_1's rmse: 0.843967\n",
      "[188]\ttraining's rmse: 0.773886\tvalid_1's rmse: 0.84393\n",
      "[189]\ttraining's rmse: 0.77351\tvalid_1's rmse: 0.843823\n",
      "[190]\ttraining's rmse: 0.773315\tvalid_1's rmse: 0.843798\n",
      "[191]\ttraining's rmse: 0.773094\tvalid_1's rmse: 0.843684\n",
      "[192]\ttraining's rmse: 0.772982\tvalid_1's rmse: 0.843677\n",
      "[193]\ttraining's rmse: 0.772891\tvalid_1's rmse: 0.843645\n",
      "[194]\ttraining's rmse: 0.772797\tvalid_1's rmse: 0.843638\n",
      "[195]\ttraining's rmse: 0.772537\tvalid_1's rmse: 0.843621\n",
      "[196]\ttraining's rmse: 0.772415\tvalid_1's rmse: 0.843599\n",
      "[197]\ttraining's rmse: 0.772296\tvalid_1's rmse: 0.84364\n",
      "[198]\ttraining's rmse: 0.772209\tvalid_1's rmse: 0.843676\n",
      "[199]\ttraining's rmse: 0.772052\tvalid_1's rmse: 0.8437\n",
      "[200]\ttraining's rmse: 0.771825\tvalid_1's rmse: 0.843511\n",
      "[201]\ttraining's rmse: 0.771707\tvalid_1's rmse: 0.843505\n",
      "[202]\ttraining's rmse: 0.771438\tvalid_1's rmse: 0.843379\n",
      "[203]\ttraining's rmse: 0.771305\tvalid_1's rmse: 0.843346\n",
      "[204]\ttraining's rmse: 0.771127\tvalid_1's rmse: 0.843559\n",
      "[205]\ttraining's rmse: 0.771027\tvalid_1's rmse: 0.843541\n",
      "[206]\ttraining's rmse: 0.770959\tvalid_1's rmse: 0.843642\n",
      "[207]\ttraining's rmse: 0.770763\tvalid_1's rmse: 0.84356\n",
      "[208]\ttraining's rmse: 0.770669\tvalid_1's rmse: 0.843606\n",
      "[209]\ttraining's rmse: 0.770587\tvalid_1's rmse: 0.84362\n",
      "[210]\ttraining's rmse: 0.770467\tvalid_1's rmse: 0.84363\n",
      "[211]\ttraining's rmse: 0.770355\tvalid_1's rmse: 0.843622\n",
      "[212]\ttraining's rmse: 0.770218\tvalid_1's rmse: 0.843513\n",
      "[213]\ttraining's rmse: 0.770049\tvalid_1's rmse: 0.843513\n",
      "[214]\ttraining's rmse: 0.769997\tvalid_1's rmse: 0.84349\n",
      "[215]\ttraining's rmse: 0.769922\tvalid_1's rmse: 0.84348\n",
      "[216]\ttraining's rmse: 0.769816\tvalid_1's rmse: 0.843432\n",
      "[217]\ttraining's rmse: 0.769683\tvalid_1's rmse: 0.843459\n",
      "[218]\ttraining's rmse: 0.769515\tvalid_1's rmse: 0.843355\n",
      "[219]\ttraining's rmse: 0.769411\tvalid_1's rmse: 0.843362\n",
      "[220]\ttraining's rmse: 0.769292\tvalid_1's rmse: 0.843422\n",
      "[221]\ttraining's rmse: 0.769151\tvalid_1's rmse: 0.843511\n",
      "[222]\ttraining's rmse: 0.769105\tvalid_1's rmse: 0.843534\n",
      "[223]\ttraining's rmse: 0.768935\tvalid_1's rmse: 0.843521\n",
      "[224]\ttraining's rmse: 0.768816\tvalid_1's rmse: 0.843544\n",
      "[225]\ttraining's rmse: 0.76867\tvalid_1's rmse: 0.843564\n",
      "[226]\ttraining's rmse: 0.768553\tvalid_1's rmse: 0.843646\n",
      "[227]\ttraining's rmse: 0.768419\tvalid_1's rmse: 0.843625\n",
      "[228]\ttraining's rmse: 0.768247\tvalid_1's rmse: 0.843535\n",
      "[229]\ttraining's rmse: 0.768179\tvalid_1's rmse: 0.843618\n",
      "[230]\ttraining's rmse: 0.768114\tvalid_1's rmse: 0.843639\n",
      "[231]\ttraining's rmse: 0.76806\tvalid_1's rmse: 0.843686\n",
      "[232]\ttraining's rmse: 0.767855\tvalid_1's rmse: 0.843662\n",
      "[233]\ttraining's rmse: 0.767616\tvalid_1's rmse: 0.843554\n",
      "[234]\ttraining's rmse: 0.767525\tvalid_1's rmse: 0.843603\n",
      "[235]\ttraining's rmse: 0.767455\tvalid_1's rmse: 0.843582\n",
      "[236]\ttraining's rmse: 0.767279\tvalid_1's rmse: 0.84357\n",
      "[237]\ttraining's rmse: 0.767119\tvalid_1's rmse: 0.843565\n",
      "[238]\ttraining's rmse: 0.76699\tvalid_1's rmse: 0.843532\n",
      "[239]\ttraining's rmse: 0.766851\tvalid_1's rmse: 0.843491\n",
      "[240]\ttraining's rmse: 0.76679\tvalid_1's rmse: 0.84348\n",
      "[241]\ttraining's rmse: 0.766733\tvalid_1's rmse: 0.843472\n",
      "[242]\ttraining's rmse: 0.766669\tvalid_1's rmse: 0.843507\n",
      "[243]\ttraining's rmse: 0.766613\tvalid_1's rmse: 0.843536\n",
      "[244]\ttraining's rmse: 0.766512\tvalid_1's rmse: 0.843621\n",
      "[245]\ttraining's rmse: 0.766427\tvalid_1's rmse: 0.843699\n",
      "[246]\ttraining's rmse: 0.766342\tvalid_1's rmse: 0.843764\n",
      "[247]\ttraining's rmse: 0.766246\tvalid_1's rmse: 0.843853\n",
      "[248]\ttraining's rmse: 0.766172\tvalid_1's rmse: 0.843823\n",
      "[249]\ttraining's rmse: 0.766085\tvalid_1's rmse: 0.843713\n",
      "[250]\ttraining's rmse: 0.765953\tvalid_1's rmse: 0.843659\n",
      "[251]\ttraining's rmse: 0.76584\tvalid_1's rmse: 0.843383\n",
      "[252]\ttraining's rmse: 0.765653\tvalid_1's rmse: 0.843356\n",
      "[253]\ttraining's rmse: 0.765486\tvalid_1's rmse: 0.843312\n",
      "[254]\ttraining's rmse: 0.765399\tvalid_1's rmse: 0.843273\n",
      "[255]\ttraining's rmse: 0.765346\tvalid_1's rmse: 0.843281\n",
      "[256]\ttraining's rmse: 0.765177\tvalid_1's rmse: 0.843287\n",
      "[257]\ttraining's rmse: 0.765075\tvalid_1's rmse: 0.843333\n",
      "[258]\ttraining's rmse: 0.76498\tvalid_1's rmse: 0.843321\n",
      "[259]\ttraining's rmse: 0.764878\tvalid_1's rmse: 0.843294\n",
      "[260]\ttraining's rmse: 0.76467\tvalid_1's rmse: 0.843435\n",
      "[261]\ttraining's rmse: 0.764602\tvalid_1's rmse: 0.84322\n",
      "[262]\ttraining's rmse: 0.764503\tvalid_1's rmse: 0.843258\n",
      "[263]\ttraining's rmse: 0.764401\tvalid_1's rmse: 0.843226\n",
      "[264]\ttraining's rmse: 0.764287\tvalid_1's rmse: 0.843219\n",
      "[265]\ttraining's rmse: 0.7642\tvalid_1's rmse: 0.843209\n",
      "[266]\ttraining's rmse: 0.764148\tvalid_1's rmse: 0.843249\n",
      "[267]\ttraining's rmse: 0.763998\tvalid_1's rmse: 0.84323\n",
      "[268]\ttraining's rmse: 0.763939\tvalid_1's rmse: 0.84325\n",
      "[269]\ttraining's rmse: 0.763901\tvalid_1's rmse: 0.843233\n",
      "[270]\ttraining's rmse: 0.763849\tvalid_1's rmse: 0.843267\n",
      "[271]\ttraining's rmse: 0.763644\tvalid_1's rmse: 0.843197\n",
      "[272]\ttraining's rmse: 0.763561\tvalid_1's rmse: 0.843173\n",
      "[273]\ttraining's rmse: 0.763342\tvalid_1's rmse: 0.843159\n",
      "[274]\ttraining's rmse: 0.763253\tvalid_1's rmse: 0.843197\n",
      "[275]\ttraining's rmse: 0.763104\tvalid_1's rmse: 0.843111\n",
      "[276]\ttraining's rmse: 0.76302\tvalid_1's rmse: 0.843094\n",
      "[277]\ttraining's rmse: 0.762872\tvalid_1's rmse: 0.843109\n",
      "[278]\ttraining's rmse: 0.762794\tvalid_1's rmse: 0.843097\n",
      "[279]\ttraining's rmse: 0.762726\tvalid_1's rmse: 0.843153\n",
      "[280]\ttraining's rmse: 0.762649\tvalid_1's rmse: 0.843136\n",
      "[281]\ttraining's rmse: 0.76259\tvalid_1's rmse: 0.843189\n",
      "[282]\ttraining's rmse: 0.762526\tvalid_1's rmse: 0.843195\n",
      "[283]\ttraining's rmse: 0.762439\tvalid_1's rmse: 0.843212\n",
      "[284]\ttraining's rmse: 0.762293\tvalid_1's rmse: 0.843134\n",
      "[285]\ttraining's rmse: 0.762143\tvalid_1's rmse: 0.843161\n",
      "[286]\ttraining's rmse: 0.762097\tvalid_1's rmse: 0.843227\n",
      "[287]\ttraining's rmse: 0.762033\tvalid_1's rmse: 0.843223\n",
      "[288]\ttraining's rmse: 0.761843\tvalid_1's rmse: 0.843365\n",
      "[289]\ttraining's rmse: 0.761761\tvalid_1's rmse: 0.843368\n",
      "[290]\ttraining's rmse: 0.761664\tvalid_1's rmse: 0.843351\n",
      "[291]\ttraining's rmse: 0.761573\tvalid_1's rmse: 0.843217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[292]\ttraining's rmse: 0.761505\tvalid_1's rmse: 0.843151\n",
      "[293]\ttraining's rmse: 0.761439\tvalid_1's rmse: 0.843167\n",
      "[294]\ttraining's rmse: 0.761374\tvalid_1's rmse: 0.843185\n",
      "[295]\ttraining's rmse: 0.7613\tvalid_1's rmse: 0.843205\n",
      "[296]\ttraining's rmse: 0.76122\tvalid_1's rmse: 0.843216\n",
      "[297]\ttraining's rmse: 0.761086\tvalid_1's rmse: 0.843219\n",
      "[298]\ttraining's rmse: 0.76097\tvalid_1's rmse: 0.843281\n",
      "[299]\ttraining's rmse: 0.760878\tvalid_1's rmse: 0.843335\n",
      "[300]\ttraining's rmse: 0.760806\tvalid_1's rmse: 0.843299\n",
      "[301]\ttraining's rmse: 0.760708\tvalid_1's rmse: 0.843304\n",
      "[302]\ttraining's rmse: 0.760619\tvalid_1's rmse: 0.843289\n",
      "[303]\ttraining's rmse: 0.760582\tvalid_1's rmse: 0.843141\n",
      "[304]\ttraining's rmse: 0.760493\tvalid_1's rmse: 0.843161\n",
      "[305]\ttraining's rmse: 0.760339\tvalid_1's rmse: 0.843121\n",
      "[306]\ttraining's rmse: 0.760202\tvalid_1's rmse: 0.843196\n",
      "[307]\ttraining's rmse: 0.760132\tvalid_1's rmse: 0.843254\n",
      "[308]\ttraining's rmse: 0.760057\tvalid_1's rmse: 0.843256\n",
      "[309]\ttraining's rmse: 0.760002\tvalid_1's rmse: 0.843275\n",
      "[310]\ttraining's rmse: 0.759896\tvalid_1's rmse: 0.843276\n",
      "[311]\ttraining's rmse: 0.759767\tvalid_1's rmse: 0.843208\n",
      "[312]\ttraining's rmse: 0.75972\tvalid_1's rmse: 0.843217\n",
      "[313]\ttraining's rmse: 0.759666\tvalid_1's rmse: 0.843204\n",
      "[314]\ttraining's rmse: 0.75961\tvalid_1's rmse: 0.843201\n",
      "[315]\ttraining's rmse: 0.759565\tvalid_1's rmse: 0.843204\n",
      "[316]\ttraining's rmse: 0.759499\tvalid_1's rmse: 0.843168\n",
      "[317]\ttraining's rmse: 0.759454\tvalid_1's rmse: 0.843113\n",
      "[318]\ttraining's rmse: 0.759337\tvalid_1's rmse: 0.84307\n",
      "[319]\ttraining's rmse: 0.759253\tvalid_1's rmse: 0.843008\n",
      "[320]\ttraining's rmse: 0.759188\tvalid_1's rmse: 0.842997\n",
      "[321]\ttraining's rmse: 0.759056\tvalid_1's rmse: 0.843058\n",
      "[322]\ttraining's rmse: 0.758896\tvalid_1's rmse: 0.843045\n",
      "[323]\ttraining's rmse: 0.758726\tvalid_1's rmse: 0.842896\n",
      "[324]\ttraining's rmse: 0.758535\tvalid_1's rmse: 0.842866\n",
      "[325]\ttraining's rmse: 0.75844\tvalid_1's rmse: 0.842871\n",
      "[326]\ttraining's rmse: 0.758337\tvalid_1's rmse: 0.842844\n",
      "[327]\ttraining's rmse: 0.758252\tvalid_1's rmse: 0.842818\n",
      "[328]\ttraining's rmse: 0.758108\tvalid_1's rmse: 0.842739\n",
      "[329]\ttraining's rmse: 0.757963\tvalid_1's rmse: 0.842684\n",
      "[330]\ttraining's rmse: 0.757873\tvalid_1's rmse: 0.842774\n",
      "[331]\ttraining's rmse: 0.757791\tvalid_1's rmse: 0.842805\n",
      "[332]\ttraining's rmse: 0.757671\tvalid_1's rmse: 0.842765\n",
      "[333]\ttraining's rmse: 0.757539\tvalid_1's rmse: 0.842735\n",
      "[334]\ttraining's rmse: 0.757465\tvalid_1's rmse: 0.842725\n",
      "[335]\ttraining's rmse: 0.757363\tvalid_1's rmse: 0.842676\n",
      "[336]\ttraining's rmse: 0.757292\tvalid_1's rmse: 0.842713\n",
      "[337]\ttraining's rmse: 0.757215\tvalid_1's rmse: 0.842597\n",
      "[338]\ttraining's rmse: 0.757151\tvalid_1's rmse: 0.842643\n",
      "[339]\ttraining's rmse: 0.757088\tvalid_1's rmse: 0.842516\n",
      "[340]\ttraining's rmse: 0.757\tvalid_1's rmse: 0.842573\n",
      "[341]\ttraining's rmse: 0.75682\tvalid_1's rmse: 0.842701\n",
      "[342]\ttraining's rmse: 0.756706\tvalid_1's rmse: 0.842753\n",
      "[343]\ttraining's rmse: 0.756649\tvalid_1's rmse: 0.842748\n",
      "[344]\ttraining's rmse: 0.756578\tvalid_1's rmse: 0.842757\n",
      "[345]\ttraining's rmse: 0.756464\tvalid_1's rmse: 0.842748\n",
      "[346]\ttraining's rmse: 0.756389\tvalid_1's rmse: 0.84275\n",
      "[347]\ttraining's rmse: 0.75633\tvalid_1's rmse: 0.84275\n",
      "[348]\ttraining's rmse: 0.756275\tvalid_1's rmse: 0.842764\n",
      "[349]\ttraining's rmse: 0.756231\tvalid_1's rmse: 0.842783\n",
      "[350]\ttraining's rmse: 0.756189\tvalid_1's rmse: 0.842837\n",
      "[351]\ttraining's rmse: 0.75613\tvalid_1's rmse: 0.8428\n",
      "[352]\ttraining's rmse: 0.75607\tvalid_1's rmse: 0.84282\n",
      "[353]\ttraining's rmse: 0.755958\tvalid_1's rmse: 0.842682\n",
      "[354]\ttraining's rmse: 0.755909\tvalid_1's rmse: 0.842717\n",
      "[355]\ttraining's rmse: 0.755852\tvalid_1's rmse: 0.842704\n",
      "[356]\ttraining's rmse: 0.755803\tvalid_1's rmse: 0.842682\n",
      "[357]\ttraining's rmse: 0.755716\tvalid_1's rmse: 0.842659\n",
      "[358]\ttraining's rmse: 0.755658\tvalid_1's rmse: 0.842714\n",
      "[359]\ttraining's rmse: 0.755573\tvalid_1's rmse: 0.842736\n",
      "[360]\ttraining's rmse: 0.755519\tvalid_1's rmse: 0.842682\n",
      "[361]\ttraining's rmse: 0.755489\tvalid_1's rmse: 0.842691\n",
      "[362]\ttraining's rmse: 0.755438\tvalid_1's rmse: 0.842704\n",
      "[363]\ttraining's rmse: 0.755409\tvalid_1's rmse: 0.842712\n",
      "[364]\ttraining's rmse: 0.755272\tvalid_1's rmse: 0.842717\n",
      "[365]\ttraining's rmse: 0.755248\tvalid_1's rmse: 0.8427\n",
      "[366]\ttraining's rmse: 0.755151\tvalid_1's rmse: 0.842683\n",
      "[367]\ttraining's rmse: 0.755074\tvalid_1's rmse: 0.842682\n",
      "[368]\ttraining's rmse: 0.755023\tvalid_1's rmse: 0.842668\n",
      "[369]\ttraining's rmse: 0.754858\tvalid_1's rmse: 0.842816\n",
      "[370]\ttraining's rmse: 0.75482\tvalid_1's rmse: 0.842851\n",
      "[371]\ttraining's rmse: 0.754775\tvalid_1's rmse: 0.842849\n",
      "[372]\ttraining's rmse: 0.754713\tvalid_1's rmse: 0.842819\n",
      "[373]\ttraining's rmse: 0.754665\tvalid_1's rmse: 0.8428\n",
      "[374]\ttraining's rmse: 0.754625\tvalid_1's rmse: 0.842819\n",
      "[375]\ttraining's rmse: 0.754562\tvalid_1's rmse: 0.842836\n",
      "[376]\ttraining's rmse: 0.7545\tvalid_1's rmse: 0.842846\n",
      "[377]\ttraining's rmse: 0.754398\tvalid_1's rmse: 0.842825\n",
      "[378]\ttraining's rmse: 0.754341\tvalid_1's rmse: 0.8428\n",
      "[379]\ttraining's rmse: 0.75429\tvalid_1's rmse: 0.842802\n",
      "[380]\ttraining's rmse: 0.754229\tvalid_1's rmse: 0.842813\n",
      "[381]\ttraining's rmse: 0.754189\tvalid_1's rmse: 0.842818\n",
      "[382]\ttraining's rmse: 0.754133\tvalid_1's rmse: 0.842812\n",
      "[383]\ttraining's rmse: 0.754083\tvalid_1's rmse: 0.842842\n",
      "[384]\ttraining's rmse: 0.754027\tvalid_1's rmse: 0.842844\n",
      "[385]\ttraining's rmse: 0.753961\tvalid_1's rmse: 0.842908\n",
      "[386]\ttraining's rmse: 0.753924\tvalid_1's rmse: 0.842914\n",
      "[387]\ttraining's rmse: 0.753884\tvalid_1's rmse: 0.842924\n",
      "[388]\ttraining's rmse: 0.753814\tvalid_1's rmse: 0.842922\n",
      "[389]\ttraining's rmse: 0.753745\tvalid_1's rmse: 0.842973\n",
      "[390]\ttraining's rmse: 0.753663\tvalid_1's rmse: 0.843003\n",
      "[391]\ttraining's rmse: 0.753633\tvalid_1's rmse: 0.843011\n",
      "[392]\ttraining's rmse: 0.7536\tvalid_1's rmse: 0.843013\n",
      "[393]\ttraining's rmse: 0.753531\tvalid_1's rmse: 0.843013\n",
      "[394]\ttraining's rmse: 0.753457\tvalid_1's rmse: 0.84303\n",
      "[395]\ttraining's rmse: 0.753383\tvalid_1's rmse: 0.84301\n",
      "[396]\ttraining's rmse: 0.753313\tvalid_1's rmse: 0.84305\n",
      "[397]\ttraining's rmse: 0.753252\tvalid_1's rmse: 0.843079\n",
      "[398]\ttraining's rmse: 0.753181\tvalid_1's rmse: 0.843086\n",
      "[399]\ttraining's rmse: 0.753118\tvalid_1's rmse: 0.843061\n",
      "Early stopping, best iteration is:\n",
      "[339]\ttraining's rmse: 0.757088\tvalid_1's rmse: 0.842516\n",
      "total rmse : 0.8425161900015496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.8425161900015496"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HyperParameters found using Bayesian optimization (see below section)\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'num_leaves': 116,\n",
    "    'max_depth': 7,\n",
    "    'feature_fraction': 0.9674089710795843,\n",
    "    'bagging_fraction': 0.9064002765231052,\n",
    "    'bagging_freq': 3,\n",
    "    'learning_rate': 0.05096799128425091,\n",
    "    'early_stopping_round': 60,\n",
    "    'lambda': 0.9941443549841082,\n",
    "    'min_split_gain': 7.324275428951591,\n",
    "    'min_child_samples': 107,\n",
    "    'min_data_per_group': 20,\n",
    "    'min_child_weight': 9.558708559566462,\n",
    "    'cat_smooth': 2.876702721398722,\n",
    "    'min_data_in_leaf': 184,\n",
    "    'max_bin': 341,\n",
    "    'verbosity' : -1\n",
    "}\n",
    "run_lgb(features,categorical_features,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train one last time on the whole dataset before submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 60 rounds.\n",
      "[10]\ttraining's rmse: 0.98564\n",
      "[20]\ttraining's rmse: 0.895203\n",
      "[30]\ttraining's rmse: 0.8535\n",
      "[40]\ttraining's rmse: 0.832808\n",
      "[50]\ttraining's rmse: 0.820881\n",
      "[60]\ttraining's rmse: 0.814011\n",
      "[70]\ttraining's rmse: 0.808419\n",
      "[80]\ttraining's rmse: 0.804365\n",
      "[90]\ttraining's rmse: 0.80107\n",
      "[100]\ttraining's rmse: 0.798154\n",
      "[110]\ttraining's rmse: 0.795268\n",
      "[120]\ttraining's rmse: 0.792629\n",
      "[130]\ttraining's rmse: 0.790126\n",
      "[140]\ttraining's rmse: 0.787491\n",
      "[150]\ttraining's rmse: 0.785591\n",
      "[160]\ttraining's rmse: 0.783866\n",
      "[170]\ttraining's rmse: 0.782401\n",
      "[180]\ttraining's rmse: 0.780607\n",
      "[190]\ttraining's rmse: 0.778943\n",
      "[200]\ttraining's rmse: 0.777715\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttraining's rmse: 0.777715\n"
     ]
    }
   ],
   "source": [
    "ROUNDS = 200#Found by averaging the optimal nb of rounds for predictions of month 32 and 33\n",
    "traindf = sales[(sales.month <= 33) & (sales.month >= 12)]\n",
    "traingbm = lgb.Dataset(traindf[features], label=traindf['target'], categorical_feature=categorical_features, free_raw_data=False)\n",
    "m = lgb.train(params, traingbm, ROUNDS, valid_sets=[traingbm], verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_importance(model):\n",
    "    gain = model.feature_importance('gain')\n",
    "    feat_imp = pd.DataFrame(\n",
    "        {\n",
    "            'feature':model.feature_name(), \n",
    "            'split':model.feature_importance('split'), \n",
    "            'gain':100 * gain / gain.sum()\n",
    "        }\n",
    "    ).sort_values('gain', ascending=False)\n",
    "    return feat_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>split</th>\n",
       "      <th>gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>tsshopiditemid1</td>\n",
       "      <td>670</td>\n",
       "      <td>45.212525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>meanlagitemshop</td>\n",
       "      <td>505</td>\n",
       "      <td>15.694011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>itemcategoryid</td>\n",
       "      <td>2759</td>\n",
       "      <td>9.858314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meanlagitem</td>\n",
       "      <td>1498</td>\n",
       "      <td>4.963407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>newitem</td>\n",
       "      <td>224</td>\n",
       "      <td>4.374682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shopid</td>\n",
       "      <td>2147</td>\n",
       "      <td>3.280105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>monthofyear</td>\n",
       "      <td>925</td>\n",
       "      <td>1.967077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tsitemcategorysubtypeidshopid1</td>\n",
       "      <td>290</td>\n",
       "      <td>1.811962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>tsitemid2</td>\n",
       "      <td>792</td>\n",
       "      <td>1.712440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cumulativemeanitemshop</td>\n",
       "      <td>376</td>\n",
       "      <td>1.423727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>tsitemcategoryidshopid1</td>\n",
       "      <td>178</td>\n",
       "      <td>1.373073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>tsshopid12</td>\n",
       "      <td>311</td>\n",
       "      <td>1.101707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ratiolastavgprice</td>\n",
       "      <td>692</td>\n",
       "      <td>0.764207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>itemcategorysubtypeid</td>\n",
       "      <td>226</td>\n",
       "      <td>0.694088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ratiolastminprice</td>\n",
       "      <td>511</td>\n",
       "      <td>0.567828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ratiolastmaxprice</td>\n",
       "      <td>482</td>\n",
       "      <td>0.535950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>month</td>\n",
       "      <td>479</td>\n",
       "      <td>0.532773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>tsitemcategoryid1</td>\n",
       "      <td>330</td>\n",
       "      <td>0.420726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tsitemid1</td>\n",
       "      <td>205</td>\n",
       "      <td>0.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>tsshopiditemid6</td>\n",
       "      <td>135</td>\n",
       "      <td>0.363314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>tsitemid3</td>\n",
       "      <td>417</td>\n",
       "      <td>0.337772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>tsitemcategorysubtypeid1</td>\n",
       "      <td>261</td>\n",
       "      <td>0.294307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ratiolastanteprice</td>\n",
       "      <td>336</td>\n",
       "      <td>0.275725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tsshopid6</td>\n",
       "      <td>85</td>\n",
       "      <td>0.199646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>tsshopiditemid3</td>\n",
       "      <td>69</td>\n",
       "      <td>0.198898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>tsitemcategorytypeidshopid1</td>\n",
       "      <td>115</td>\n",
       "      <td>0.139624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>tsitemcategorytypeidshoptypeid1</td>\n",
       "      <td>146</td>\n",
       "      <td>0.128486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tsshoptypeid1</td>\n",
       "      <td>120</td>\n",
       "      <td>0.084841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>tsitemid12</td>\n",
       "      <td>147</td>\n",
       "      <td>0.084614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tsshopid2</td>\n",
       "      <td>73</td>\n",
       "      <td>0.083502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tsshopid3</td>\n",
       "      <td>74</td>\n",
       "      <td>0.078920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nbweekends</td>\n",
       "      <td>84</td>\n",
       "      <td>0.074420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>itemcategorytypeid</td>\n",
       "      <td>28</td>\n",
       "      <td>0.061331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>tsitemcategorysubtypeidshoptypeid1</td>\n",
       "      <td>58</td>\n",
       "      <td>0.044002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tsshopcityid1</td>\n",
       "      <td>77</td>\n",
       "      <td>0.042850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>shopcityid</td>\n",
       "      <td>63</td>\n",
       "      <td>0.042666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>tsitemcategorytypeidshopcityid1</td>\n",
       "      <td>75</td>\n",
       "      <td>0.040904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tsshopid1</td>\n",
       "      <td>64</td>\n",
       "      <td>0.039864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>tsshopcityidshoptypeid1</td>\n",
       "      <td>57</td>\n",
       "      <td>0.033482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>tsitemcategoryidshopcityid1</td>\n",
       "      <td>48</td>\n",
       "      <td>0.032886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>tsitemcategoryidshoptypeid1</td>\n",
       "      <td>39</td>\n",
       "      <td>0.028660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>tsitemcategorysubtypeidshopcityid1</td>\n",
       "      <td>42</td>\n",
       "      <td>0.026970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>tsshopiditemid12</td>\n",
       "      <td>69</td>\n",
       "      <td>0.026660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>tsitemcategoryiditemcategorytypeid1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.003948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>tsitemcategorytypeiditemcategorysubtypeid1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.003829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>shoptypeid</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>year</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>tsshoptypeidshopid1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>tsitemcategoryiditemcategorysubtypeid1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>tsshopcityidshopid1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       feature  split       gain\n",
       "48                             tsshopiditemid1    670  45.212525\n",
       "3                              meanlagitemshop    505  15.694011\n",
       "2                               itemcategoryid   2759   9.858314\n",
       "4                                  meanlagitem   1498   4.963407\n",
       "18                                     newitem    224   4.374682\n",
       "1                                       shopid   2147   3.280105\n",
       "12                                 monthofyear    925   1.967077\n",
       "39              tsitemcategorysubtypeidshopid1    290   1.811962\n",
       "44                                   tsitemid2    792   1.712440\n",
       "5                       cumulativemeanitemshop    376   1.423727\n",
       "32                     tsitemcategoryidshopid1    178   1.373073\n",
       "23                                  tsshopid12    311   1.101707\n",
       "15                           ratiolastavgprice    692   0.764207\n",
       "9                        itemcategorysubtypeid    226   0.694088\n",
       "16                           ratiolastminprice    511   0.567828\n",
       "17                           ratiolastmaxprice    482   0.535950\n",
       "0                                        month    479   0.532773\n",
       "26                           tsitemcategoryid1    330   0.420726\n",
       "43                                   tsitemid1    205   0.391000\n",
       "51                             tsshopiditemid6    135   0.363314\n",
       "45                                   tsitemid3    417   0.337772\n",
       "27                    tsitemcategorysubtypeid1    261   0.294307\n",
       "14                          ratiolastanteprice    336   0.275725\n",
       "22                                   tsshopid6     85   0.199646\n",
       "50                             tsshopiditemid3     69   0.198898\n",
       "..                                         ...    ...        ...\n",
       "36                 tsitemcategorytypeidshopid1    115   0.139624\n",
       "35             tsitemcategorytypeidshoptypeid1    146   0.128486\n",
       "25                               tsshoptypeid1    120   0.084841\n",
       "47                                  tsitemid12    147   0.084614\n",
       "20                                   tsshopid2     73   0.083502\n",
       "21                                   tsshopid3     74   0.078920\n",
       "13                                  nbweekends     84   0.074420\n",
       "8                           itemcategorytypeid     28   0.061331\n",
       "38          tsitemcategorysubtypeidshoptypeid1     58   0.044002\n",
       "24                               tsshopcityid1     77   0.042850\n",
       "6                                   shopcityid     63   0.042666\n",
       "34             tsitemcategorytypeidshopcityid1     75   0.040904\n",
       "19                                   tsshopid1     64   0.039864\n",
       "40                     tsshopcityidshoptypeid1     57   0.033482\n",
       "30                 tsitemcategoryidshopcityid1     48   0.032886\n",
       "31                 tsitemcategoryidshoptypeid1     39   0.028660\n",
       "37          tsitemcategorysubtypeidshopcityid1     42   0.026970\n",
       "52                            tsshopiditemid12     69   0.026660\n",
       "28         tsitemcategoryiditemcategorytypeid1      5   0.003948\n",
       "33  tsitemcategorytypeiditemcategorysubtypeid1     10   0.003829\n",
       "7                                   shoptypeid      0   0.000000\n",
       "11                                        year      0   0.000000\n",
       "42                         tsshoptypeidshopid1      0   0.000000\n",
       "29      tsitemcategoryiditemcategorysubtypeid1      0   0.000000\n",
       "41                         tsshopcityidshopid1      0   0.000000\n",
       "\n",
       "[53 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_features_importance(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x10a93b750>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.save_model('lgb.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = lgb.Booster(model_file='lgb.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>shopid</th>\n",
       "      <th>itemid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  shopid  itemid\n",
       "0   0       5    5037"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('./data/test.csv')\n",
    "test.columns = ['ID','shopid','itemid']\n",
    "test.loc[test.shopid == 0, 'shopid'] = 57\n",
    "test.loc[test.shopid == 1, 'shopid'] = 58\n",
    "test.loc[test.shopid == 10, 'shopid'] = 11\n",
    "test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214200, 2)\n",
      "-0.7244908667897672\n",
      "20.128192412081304\n",
      "0.0\n",
      "20.0\n"
     ]
    }
   ],
   "source": [
    "testdf = sales[sales.month == 34]\n",
    "preds = m.predict(testdf[features])\n",
    "testdf['item_cnt_month'] = preds\n",
    "submission = pd.merge(test,testdf[['shopid','itemid','item_cnt_month']],on=['shopid','itemid'],how='left')\n",
    "submission = submission[['ID','item_cnt_month']]\n",
    "print(submission.shape)\n",
    "print(submission['item_cnt_month'].min())\n",
    "print(submission['item_cnt_month'].max())\n",
    "submission['item_cnt_month'] = submission['item_cnt_month'].clip(0,20)\n",
    "print(submission['item_cnt_month'].min())\n",
    "print(submission['item_cnt_month'].max())\n",
    "\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Kaggle Ranking 2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Kaggle ranking 1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find good hyperparameters using Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_lgb(num_leaves, max_depth, min_child_weight, feature_fraction, bagging_fraction, bagging_freq, learning_rate, lambda_param, min_split_gain, min_child_samples, min_data_per_group, cat_smooth, min_data_in_leaf, max_bin):\n",
    "    params = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'num_leaves': int(num_leaves),\n",
    "        'max_depth': int(max_depth),\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'feature_fraction': feature_fraction,\n",
    "        'bagging_fraction': bagging_fraction, \n",
    "        'bagging_freq': int(bagging_freq),\n",
    "        'learning_rate': learning_rate,\n",
    "        'early_stopping_round': 60,\n",
    "        'lambda': lambda_param,\n",
    "        'min_split_gain': min_split_gain,\n",
    "        'min_child_samples': int(min_child_samples),\n",
    "        'min_data_per_group': int(min_data_per_group),\n",
    "        'cat_smooth': cat_smooth,\n",
    "        'min_data_in_leaf': int(min_data_in_leaf),\n",
    "        'max_bin': int(max_bin),\n",
    "        'verbosity' : -1\n",
    "    }\n",
    "    score = run_lgb(features,categorical_features,params)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | baggin... | cat_sm... | featur... | lambda... | learni... |  max_bin  | max_depth | min_ch... | min_ch... | min_da... | min_da... | min_sp... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[1]\ttraining's rmse: 1.12509\tvalid_1's rmse: 1.08905\n",
      "Training until validation scores don't improve for 60 rounds.\n",
      "[2]\ttraining's rmse: 1.07154\tvalid_1's rmse: 1.04815\n",
      "[3]\ttraining's rmse: 1.02854\tvalid_1's rmse: 1.01589\n",
      "[4]\ttraining's rmse: 0.992647\tvalid_1's rmse: 0.991493\n",
      "[5]\ttraining's rmse: 0.963905\tvalid_1's rmse: 0.971132\n",
      "[6]\ttraining's rmse: 0.940176\tvalid_1's rmse: 0.954991\n",
      "[7]\ttraining's rmse: 0.920797\tvalid_1's rmse: 0.942607\n",
      "[8]\ttraining's rmse: 0.904682\tvalid_1's rmse: 0.931317\n",
      "[9]\ttraining's rmse: 0.891917\tvalid_1's rmse: 0.923861\n",
      "[10]\ttraining's rmse: 0.881004\tvalid_1's rmse: 0.91683\n",
      "[11]\ttraining's rmse: 0.871762\tvalid_1's rmse: 0.911483\n",
      "[12]\ttraining's rmse: 0.864145\tvalid_1's rmse: 0.907989\n",
      "[13]\ttraining's rmse: 0.858054\tvalid_1's rmse: 0.904993\n",
      "[14]\ttraining's rmse: 0.85281\tvalid_1's rmse: 0.902128\n",
      "[15]\ttraining's rmse: 0.848401\tvalid_1's rmse: 0.900064\n",
      "[16]\ttraining's rmse: 0.844264\tvalid_1's rmse: 0.898331\n",
      "[17]\ttraining's rmse: 0.840797\tvalid_1's rmse: 0.895624\n",
      "[18]\ttraining's rmse: 0.837891\tvalid_1's rmse: 0.894672\n",
      "[19]\ttraining's rmse: 0.835261\tvalid_1's rmse: 0.893834\n",
      "[20]\ttraining's rmse: 0.832467\tvalid_1's rmse: 0.886226\n",
      "[21]\ttraining's rmse: 0.82982\tvalid_1's rmse: 0.885243\n",
      "[22]\ttraining's rmse: 0.827764\tvalid_1's rmse: 0.883589\n",
      "[23]\ttraining's rmse: 0.826118\tvalid_1's rmse: 0.883205\n",
      "[24]\ttraining's rmse: 0.824101\tvalid_1's rmse: 0.882616\n",
      "[25]\ttraining's rmse: 0.822245\tvalid_1's rmse: 0.882114\n",
      "[26]\ttraining's rmse: 0.820748\tvalid_1's rmse: 0.881787\n",
      "[27]\ttraining's rmse: 0.819338\tvalid_1's rmse: 0.881476\n",
      "[28]\ttraining's rmse: 0.817824\tvalid_1's rmse: 0.881296\n",
      "[29]\ttraining's rmse: 0.816434\tvalid_1's rmse: 0.88124\n",
      "[30]\ttraining's rmse: 0.815143\tvalid_1's rmse: 0.881369\n",
      "[31]\ttraining's rmse: 0.814097\tvalid_1's rmse: 0.882449\n",
      "[32]\ttraining's rmse: 0.81298\tvalid_1's rmse: 0.882741\n",
      "[33]\ttraining's rmse: 0.811828\tvalid_1's rmse: 0.882545\n",
      "[34]\ttraining's rmse: 0.810541\tvalid_1's rmse: 0.883155\n",
      "[35]\ttraining's rmse: 0.809577\tvalid_1's rmse: 0.883938\n",
      "[36]\ttraining's rmse: 0.808737\tvalid_1's rmse: 0.883684\n",
      "[37]\ttraining's rmse: 0.807598\tvalid_1's rmse: 0.883893\n",
      "[38]\ttraining's rmse: 0.806553\tvalid_1's rmse: 0.883447\n",
      "[39]\ttraining's rmse: 0.805861\tvalid_1's rmse: 0.883391\n",
      "[40]\ttraining's rmse: 0.805107\tvalid_1's rmse: 0.883557\n",
      "[41]\ttraining's rmse: 0.804118\tvalid_1's rmse: 0.883437\n",
      "[42]\ttraining's rmse: 0.803329\tvalid_1's rmse: 0.883246\n",
      "[43]\ttraining's rmse: 0.802293\tvalid_1's rmse: 0.882931\n",
      "[44]\ttraining's rmse: 0.80135\tvalid_1's rmse: 0.883691\n",
      "[45]\ttraining's rmse: 0.800589\tvalid_1's rmse: 0.883752\n",
      "[46]\ttraining's rmse: 0.799852\tvalid_1's rmse: 0.883594\n",
      "[47]\ttraining's rmse: 0.799143\tvalid_1's rmse: 0.884595\n",
      "[48]\ttraining's rmse: 0.797938\tvalid_1's rmse: 0.884793\n",
      "[49]\ttraining's rmse: 0.797078\tvalid_1's rmse: 0.885845\n",
      "[50]\ttraining's rmse: 0.79647\tvalid_1's rmse: 0.885645\n",
      "[51]\ttraining's rmse: 0.795856\tvalid_1's rmse: 0.885694\n",
      "[52]\ttraining's rmse: 0.795321\tvalid_1's rmse: 0.885681\n",
      "[53]\ttraining's rmse: 0.794528\tvalid_1's rmse: 0.886259\n",
      "[54]\ttraining's rmse: 0.79372\tvalid_1's rmse: 0.886075\n",
      "[55]\ttraining's rmse: 0.793101\tvalid_1's rmse: 0.885931\n",
      "[56]\ttraining's rmse: 0.792363\tvalid_1's rmse: 0.886688\n",
      "[57]\ttraining's rmse: 0.791906\tvalid_1's rmse: 0.886481\n",
      "[58]\ttraining's rmse: 0.79135\tvalid_1's rmse: 0.886633\n",
      "[59]\ttraining's rmse: 0.790593\tvalid_1's rmse: 0.88652\n",
      "[60]\ttraining's rmse: 0.790167\tvalid_1's rmse: 0.885776\n",
      "[61]\ttraining's rmse: 0.789675\tvalid_1's rmse: 0.885607\n",
      "[62]\ttraining's rmse: 0.78917\tvalid_1's rmse: 0.885321\n",
      "[63]\ttraining's rmse: 0.788739\tvalid_1's rmse: 0.885319\n",
      "[64]\ttraining's rmse: 0.788368\tvalid_1's rmse: 0.885269\n",
      "[65]\ttraining's rmse: 0.787868\tvalid_1's rmse: 0.8853\n",
      "[66]\ttraining's rmse: 0.787123\tvalid_1's rmse: 0.885688\n",
      "[67]\ttraining's rmse: 0.786766\tvalid_1's rmse: 0.88567\n",
      "[68]\ttraining's rmse: 0.786279\tvalid_1's rmse: 0.888667\n",
      "[69]\ttraining's rmse: 0.785888\tvalid_1's rmse: 0.888524\n",
      "[70]\ttraining's rmse: 0.785478\tvalid_1's rmse: 0.888102\n",
      "[71]\ttraining's rmse: 0.785026\tvalid_1's rmse: 0.887936\n",
      "[72]\ttraining's rmse: 0.784493\tvalid_1's rmse: 0.887903\n",
      "[73]\ttraining's rmse: 0.784033\tvalid_1's rmse: 0.888325\n",
      "[74]\ttraining's rmse: 0.783674\tvalid_1's rmse: 0.888469\n",
      "[75]\ttraining's rmse: 0.783266\tvalid_1's rmse: 0.888583\n",
      "[76]\ttraining's rmse: 0.782709\tvalid_1's rmse: 0.896966\n",
      "[77]\ttraining's rmse: 0.782332\tvalid_1's rmse: 0.89684\n",
      "[78]\ttraining's rmse: 0.781925\tvalid_1's rmse: 0.89692\n",
      "[79]\ttraining's rmse: 0.781448\tvalid_1's rmse: 0.896851\n",
      "[80]\ttraining's rmse: 0.780959\tvalid_1's rmse: 0.896801\n",
      "[81]\ttraining's rmse: 0.780671\tvalid_1's rmse: 0.897201\n",
      "[82]\ttraining's rmse: 0.780345\tvalid_1's rmse: 0.897106\n",
      "[83]\ttraining's rmse: 0.779928\tvalid_1's rmse: 0.897096\n",
      "[84]\ttraining's rmse: 0.779474\tvalid_1's rmse: 0.897361\n",
      "[85]\ttraining's rmse: 0.778899\tvalid_1's rmse: 0.897494\n",
      "[86]\ttraining's rmse: 0.778583\tvalid_1's rmse: 0.897339\n",
      "[87]\ttraining's rmse: 0.778315\tvalid_1's rmse: 0.897277\n",
      "[88]\ttraining's rmse: 0.777792\tvalid_1's rmse: 0.897317\n",
      "[89]\ttraining's rmse: 0.777518\tvalid_1's rmse: 0.897429\n",
      "Early stopping, best iteration is:\n",
      "[29]\ttraining's rmse: 0.816434\tvalid_1's rmse: 0.88124\n",
      "total rmse : 0.8763972148816184\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.8764  \u001b[0m | \u001b[0m 0.9375  \u001b[0m | \u001b[0m 19.06   \u001b[0m | \u001b[0m 3.66    \u001b[0m | \u001b[0m 0.9599  \u001b[0m | \u001b[0m 0.312   \u001b[0m | \u001b[0m 0.1256  \u001b[0m | \u001b[0m 59.88   \u001b[0m | \u001b[0m 17.59   \u001b[0m | \u001b[0m 98.14   \u001b[0m | \u001b[0m 7.227   \u001b[0m | \u001b[0m 13.91   \u001b[0m | \u001b[0m 388.6   \u001b[0m | \u001b[0m 8.324   \u001b[0m | \u001b[0m 41.23   \u001b[0m |\n",
      "[1]\ttraining's rmse: 1.01774\tvalid_1's rmse: 1.01974\n",
      "Training until validation scores don't improve for 60 rounds.\n",
      "[2]\ttraining's rmse: 0.960798\tvalid_1's rmse: 0.980422\n",
      "[3]\ttraining's rmse: 0.943746\tvalid_1's rmse: 0.969586\n",
      "[4]\ttraining's rmse: 0.916808\tvalid_1's rmse: 0.940497\n",
      "[5]\ttraining's rmse: 0.911362\tvalid_1's rmse: 0.936663\n",
      "[6]\ttraining's rmse: 0.903406\tvalid_1's rmse: 0.935372\n",
      "[7]\ttraining's rmse: 0.898435\tvalid_1's rmse: 0.930071\n",
      "[8]\ttraining's rmse: 0.892531\tvalid_1's rmse: 0.928687\n",
      "[9]\ttraining's rmse: 0.889389\tvalid_1's rmse: 0.925989\n",
      "[10]\ttraining's rmse: 0.88684\tvalid_1's rmse: 0.924888\n",
      "[11]\ttraining's rmse: 0.885651\tvalid_1's rmse: 0.924505\n",
      "[12]\ttraining's rmse: 0.883638\tvalid_1's rmse: 0.924218\n",
      "[13]\ttraining's rmse: 0.881398\tvalid_1's rmse: 0.92319\n",
      "[14]\ttraining's rmse: 0.879224\tvalid_1's rmse: 0.922081\n",
      "[15]\ttraining's rmse: 0.876877\tvalid_1's rmse: 0.919164\n",
      "[16]\ttraining's rmse: 0.875317\tvalid_1's rmse: 0.918\n",
      "[17]\ttraining's rmse: 0.87362\tvalid_1's rmse: 0.916062\n",
      "[18]\ttraining's rmse: 0.871798\tvalid_1's rmse: 0.91475\n",
      "[19]\ttraining's rmse: 0.870378\tvalid_1's rmse: 0.914213\n",
      "[20]\ttraining's rmse: 0.86962\tvalid_1's rmse: 0.914887\n",
      "[21]\ttraining's rmse: 0.868472\tvalid_1's rmse: 0.913592\n",
      "[22]\ttraining's rmse: 0.866921\tvalid_1's rmse: 0.912857\n",
      "[23]\ttraining's rmse: 0.866295\tvalid_1's rmse: 0.913569\n",
      "[24]\ttraining's rmse: 0.865607\tvalid_1's rmse: 0.912685\n",
      "[25]\ttraining's rmse: 0.865128\tvalid_1's rmse: 0.912409\n",
      "[26]\ttraining's rmse: 0.864492\tvalid_1's rmse: 0.912794\n",
      "[27]\ttraining's rmse: 0.863605\tvalid_1's rmse: 0.91282\n",
      "[28]\ttraining's rmse: 0.863075\tvalid_1's rmse: 0.911325\n",
      "[29]\ttraining's rmse: 0.862693\tvalid_1's rmse: 0.911116\n",
      "[30]\ttraining's rmse: 0.861653\tvalid_1's rmse: 0.911148\n",
      "[31]\ttraining's rmse: 0.861293\tvalid_1's rmse: 0.911424\n",
      "[32]\ttraining's rmse: 0.860772\tvalid_1's rmse: 0.911295\n",
      "[33]\ttraining's rmse: 0.86043\tvalid_1's rmse: 0.911752\n",
      "[34]\ttraining's rmse: 0.86001\tvalid_1's rmse: 0.911731\n",
      "[35]\ttraining's rmse: 0.859685\tvalid_1's rmse: 0.911136\n",
      "[36]\ttraining's rmse: 0.859069\tvalid_1's rmse: 0.911019\n",
      "[37]\ttraining's rmse: 0.858604\tvalid_1's rmse: 0.910231\n",
      "[38]\ttraining's rmse: 0.858039\tvalid_1's rmse: 0.910977\n",
      "[39]\ttraining's rmse: 0.857583\tvalid_1's rmse: 0.910027\n",
      "[40]\ttraining's rmse: 0.855867\tvalid_1's rmse: 0.908235\n",
      "[41]\ttraining's rmse: 0.855248\tvalid_1's rmse: 0.908203\n",
      "[42]\ttraining's rmse: 0.854687\tvalid_1's rmse: 0.906747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43]\ttraining's rmse: 0.854204\tvalid_1's rmse: 0.907115\n",
      "[44]\ttraining's rmse: 0.85396\tvalid_1's rmse: 0.90641\n",
      "[45]\ttraining's rmse: 0.853455\tvalid_1's rmse: 0.905891\n",
      "[46]\ttraining's rmse: 0.853152\tvalid_1's rmse: 0.905756\n",
      "[47]\ttraining's rmse: 0.85293\tvalid_1's rmse: 0.906083\n",
      "[48]\ttraining's rmse: 0.852543\tvalid_1's rmse: 0.906161\n",
      "[49]\ttraining's rmse: 0.852205\tvalid_1's rmse: 0.906055\n",
      "[50]\ttraining's rmse: 0.851957\tvalid_1's rmse: 0.90606\n",
      "[51]\ttraining's rmse: 0.851753\tvalid_1's rmse: 0.906109\n",
      "[52]\ttraining's rmse: 0.851564\tvalid_1's rmse: 0.906089\n",
      "[53]\ttraining's rmse: 0.851177\tvalid_1's rmse: 0.906281\n",
      "[54]\ttraining's rmse: 0.850846\tvalid_1's rmse: 0.905601\n",
      "[55]\ttraining's rmse: 0.850526\tvalid_1's rmse: 0.906074\n",
      "[56]\ttraining's rmse: 0.850317\tvalid_1's rmse: 0.906415\n",
      "[57]\ttraining's rmse: 0.849918\tvalid_1's rmse: 0.905743\n",
      "[58]\ttraining's rmse: 0.849581\tvalid_1's rmse: 0.905025\n",
      "[59]\ttraining's rmse: 0.849439\tvalid_1's rmse: 0.904963\n",
      "[60]\ttraining's rmse: 0.849273\tvalid_1's rmse: 0.904956\n",
      "[61]\ttraining's rmse: 0.849065\tvalid_1's rmse: 0.904707\n",
      "[62]\ttraining's rmse: 0.848692\tvalid_1's rmse: 0.905253\n",
      "[63]\ttraining's rmse: 0.848373\tvalid_1's rmse: 0.905277\n",
      "[64]\ttraining's rmse: 0.848073\tvalid_1's rmse: 0.905382\n",
      "[65]\ttraining's rmse: 0.847848\tvalid_1's rmse: 0.905366\n",
      "[66]\ttraining's rmse: 0.847742\tvalid_1's rmse: 0.905265\n",
      "[67]\ttraining's rmse: 0.846876\tvalid_1's rmse: 0.901556\n",
      "[68]\ttraining's rmse: 0.846539\tvalid_1's rmse: 0.901445\n",
      "[69]\ttraining's rmse: 0.846231\tvalid_1's rmse: 0.901131\n",
      "[70]\ttraining's rmse: 0.845965\tvalid_1's rmse: 0.900486\n",
      "[71]\ttraining's rmse: 0.84576\tvalid_1's rmse: 0.900216\n",
      "[72]\ttraining's rmse: 0.8455\tvalid_1's rmse: 0.900295\n",
      "[73]\ttraining's rmse: 0.845247\tvalid_1's rmse: 0.899745\n",
      "[74]\ttraining's rmse: 0.844921\tvalid_1's rmse: 0.899769\n",
      "[75]\ttraining's rmse: 0.84474\tvalid_1's rmse: 0.899333\n",
      "[76]\ttraining's rmse: 0.844597\tvalid_1's rmse: 0.899225\n",
      "[77]\ttraining's rmse: 0.84437\tvalid_1's rmse: 0.899028\n",
      "[78]\ttraining's rmse: 0.844153\tvalid_1's rmse: 0.899265\n",
      "[79]\ttraining's rmse: 0.843867\tvalid_1's rmse: 0.89904\n",
      "[80]\ttraining's rmse: 0.843542\tvalid_1's rmse: 0.897528\n",
      "[81]\ttraining's rmse: 0.843342\tvalid_1's rmse: 0.897115\n",
      "[82]\ttraining's rmse: 0.843179\tvalid_1's rmse: 0.897248\n",
      "[83]\ttraining's rmse: 0.842903\tvalid_1's rmse: 0.896982\n",
      "[84]\ttraining's rmse: 0.842743\tvalid_1's rmse: 0.897006\n",
      "[85]\ttraining's rmse: 0.842486\tvalid_1's rmse: 0.897607\n",
      "[86]\ttraining's rmse: 0.842273\tvalid_1's rmse: 0.898128\n",
      "[87]\ttraining's rmse: 0.842142\tvalid_1's rmse: 0.898092\n",
      "[88]\ttraining's rmse: 0.842052\tvalid_1's rmse: 0.898089\n",
      "[89]\ttraining's rmse: 0.841898\tvalid_1's rmse: 0.897833\n",
      "[90]\ttraining's rmse: 0.841656\tvalid_1's rmse: 0.898012\n",
      "[91]\ttraining's rmse: 0.841464\tvalid_1's rmse: 0.89776\n",
      "[92]\ttraining's rmse: 0.841259\tvalid_1's rmse: 0.897834\n",
      "[93]\ttraining's rmse: 0.841158\tvalid_1's rmse: 0.897781\n",
      "[94]\ttraining's rmse: 0.84093\tvalid_1's rmse: 0.897698\n",
      "[95]\ttraining's rmse: 0.840739\tvalid_1's rmse: 0.897512\n",
      "[96]\ttraining's rmse: 0.840608\tvalid_1's rmse: 0.89728\n",
      "[97]\ttraining's rmse: 0.840479\tvalid_1's rmse: 0.897348\n",
      "[98]\ttraining's rmse: 0.840367\tvalid_1's rmse: 0.8975\n",
      "[99]\ttraining's rmse: 0.840137\tvalid_1's rmse: 0.897469\n",
      "[100]\ttraining's rmse: 0.83997\tvalid_1's rmse: 0.897293\n",
      "[101]\ttraining's rmse: 0.839766\tvalid_1's rmse: 0.897424\n",
      "[102]\ttraining's rmse: 0.839645\tvalid_1's rmse: 0.897457\n",
      "[103]\ttraining's rmse: 0.839459\tvalid_1's rmse: 0.8974\n",
      "[104]\ttraining's rmse: 0.839317\tvalid_1's rmse: 0.897968\n",
      "[105]\ttraining's rmse: 0.839147\tvalid_1's rmse: 0.897299\n",
      "[106]\ttraining's rmse: 0.839057\tvalid_1's rmse: 0.897345\n",
      "[107]\ttraining's rmse: 0.838899\tvalid_1's rmse: 0.897292\n",
      "[108]\ttraining's rmse: 0.838755\tvalid_1's rmse: 0.896925\n",
      "[109]\ttraining's rmse: 0.838506\tvalid_1's rmse: 0.897066\n",
      "[110]\ttraining's rmse: 0.838299\tvalid_1's rmse: 0.896974\n",
      "[111]\ttraining's rmse: 0.83818\tvalid_1's rmse: 0.896914\n",
      "[112]\ttraining's rmse: 0.838034\tvalid_1's rmse: 0.896241\n",
      "[113]\ttraining's rmse: 0.837911\tvalid_1's rmse: 0.896231\n",
      "[114]\ttraining's rmse: 0.83783\tvalid_1's rmse: 0.896039\n",
      "[115]\ttraining's rmse: 0.837726\tvalid_1's rmse: 0.896345\n",
      "[116]\ttraining's rmse: 0.837636\tvalid_1's rmse: 0.89649\n",
      "[117]\ttraining's rmse: 0.837559\tvalid_1's rmse: 0.896391\n",
      "[118]\ttraining's rmse: 0.837483\tvalid_1's rmse: 0.896457\n",
      "[119]\ttraining's rmse: 0.837368\tvalid_1's rmse: 0.89649\n",
      "[120]\ttraining's rmse: 0.837259\tvalid_1's rmse: 0.896528\n",
      "[121]\ttraining's rmse: 0.83705\tvalid_1's rmse: 0.896179\n",
      "[122]\ttraining's rmse: 0.836769\tvalid_1's rmse: 0.896156\n",
      "[123]\ttraining's rmse: 0.836663\tvalid_1's rmse: 0.896355\n",
      "[124]\ttraining's rmse: 0.836539\tvalid_1's rmse: 0.896258\n",
      "[125]\ttraining's rmse: 0.836399\tvalid_1's rmse: 0.896214\n",
      "[126]\ttraining's rmse: 0.836266\tvalid_1's rmse: 0.895637\n",
      "[127]\ttraining's rmse: 0.836171\tvalid_1's rmse: 0.89563\n",
      "[128]\ttraining's rmse: 0.836045\tvalid_1's rmse: 0.895318\n",
      "[129]\ttraining's rmse: 0.835909\tvalid_1's rmse: 0.8947\n",
      "[130]\ttraining's rmse: 0.835793\tvalid_1's rmse: 0.894709\n",
      "[131]\ttraining's rmse: 0.835672\tvalid_1's rmse: 0.894696\n",
      "[132]\ttraining's rmse: 0.835546\tvalid_1's rmse: 0.894563\n",
      "[133]\ttraining's rmse: 0.835428\tvalid_1's rmse: 0.894825\n",
      "[134]\ttraining's rmse: 0.833668\tvalid_1's rmse: 0.8929\n",
      "[135]\ttraining's rmse: 0.833541\tvalid_1's rmse: 0.893388\n",
      "[136]\ttraining's rmse: 0.833449\tvalid_1's rmse: 0.892951\n",
      "[137]\ttraining's rmse: 0.833376\tvalid_1's rmse: 0.892986\n",
      "[138]\ttraining's rmse: 0.83333\tvalid_1's rmse: 0.893102\n",
      "[139]\ttraining's rmse: 0.833256\tvalid_1's rmse: 0.892898\n",
      "[140]\ttraining's rmse: 0.833176\tvalid_1's rmse: 0.892919\n",
      "[141]\ttraining's rmse: 0.833057\tvalid_1's rmse: 0.89314\n",
      "[142]\ttraining's rmse: 0.832833\tvalid_1's rmse: 0.893105\n",
      "[143]\ttraining's rmse: 0.832703\tvalid_1's rmse: 0.893001\n",
      "[144]\ttraining's rmse: 0.83254\tvalid_1's rmse: 0.893561\n",
      "[145]\ttraining's rmse: 0.832436\tvalid_1's rmse: 0.893699\n",
      "[146]\ttraining's rmse: 0.832323\tvalid_1's rmse: 0.89371\n",
      "[147]\ttraining's rmse: 0.832254\tvalid_1's rmse: 0.893235\n",
      "[148]\ttraining's rmse: 0.832153\tvalid_1's rmse: 0.893229\n",
      "[149]\ttraining's rmse: 0.83212\tvalid_1's rmse: 0.893242\n",
      "[150]\ttraining's rmse: 0.832058\tvalid_1's rmse: 0.89313\n",
      "[151]\ttraining's rmse: 0.831977\tvalid_1's rmse: 0.893392\n",
      "[152]\ttraining's rmse: 0.831865\tvalid_1's rmse: 0.896162\n",
      "[153]\ttraining's rmse: 0.831804\tvalid_1's rmse: 0.895884\n",
      "[154]\ttraining's rmse: 0.831638\tvalid_1's rmse: 0.89623\n",
      "[155]\ttraining's rmse: 0.831596\tvalid_1's rmse: 0.896247\n",
      "[156]\ttraining's rmse: 0.831529\tvalid_1's rmse: 0.896341\n",
      "[157]\ttraining's rmse: 0.831442\tvalid_1's rmse: 0.896303\n",
      "[158]\ttraining's rmse: 0.831355\tvalid_1's rmse: 0.896235\n",
      "[159]\ttraining's rmse: 0.831306\tvalid_1's rmse: 0.896239\n",
      "[160]\ttraining's rmse: 0.831219\tvalid_1's rmse: 0.896224\n",
      "[161]\ttraining's rmse: 0.831147\tvalid_1's rmse: 0.896236\n",
      "[162]\ttraining's rmse: 0.831093\tvalid_1's rmse: 0.896223\n",
      "[163]\ttraining's rmse: 0.831044\tvalid_1's rmse: 0.896217\n",
      "[164]\ttraining's rmse: 0.830978\tvalid_1's rmse: 0.896227\n",
      "[165]\ttraining's rmse: 0.83089\tvalid_1's rmse: 0.896449\n",
      "[166]\ttraining's rmse: 0.830832\tvalid_1's rmse: 0.896444\n",
      "[167]\ttraining's rmse: 0.830737\tvalid_1's rmse: 0.896408\n",
      "[168]\ttraining's rmse: 0.830613\tvalid_1's rmse: 0.896677\n",
      "[169]\ttraining's rmse: 0.830537\tvalid_1's rmse: 0.896751\n",
      "[170]\ttraining's rmse: 0.83042\tvalid_1's rmse: 0.896408\n",
      "[171]\ttraining's rmse: 0.830318\tvalid_1's rmse: 0.896111\n",
      "[172]\ttraining's rmse: 0.830219\tvalid_1's rmse: 0.896335\n",
      "[173]\ttraining's rmse: 0.83008\tvalid_1's rmse: 0.896526\n",
      "[174]\ttraining's rmse: 0.830023\tvalid_1's rmse: 0.89571\n",
      "[175]\ttraining's rmse: 0.829916\tvalid_1's rmse: 0.895552\n",
      "[176]\ttraining's rmse: 0.829838\tvalid_1's rmse: 0.896033\n",
      "[177]\ttraining's rmse: 0.829712\tvalid_1's rmse: 0.895949\n",
      "[178]\ttraining's rmse: 0.829576\tvalid_1's rmse: 0.896024\n",
      "[179]\ttraining's rmse: 0.829497\tvalid_1's rmse: 0.895831\n",
      "[180]\ttraining's rmse: 0.829359\tvalid_1's rmse: 0.895929\n",
      "[181]\ttraining's rmse: 0.829286\tvalid_1's rmse: 0.895943\n",
      "[182]\ttraining's rmse: 0.829203\tvalid_1's rmse: 0.895615\n",
      "[183]\ttraining's rmse: 0.829112\tvalid_1's rmse: 0.895178\n",
      "[184]\ttraining's rmse: 0.829076\tvalid_1's rmse: 0.895174\n",
      "[185]\ttraining's rmse: 0.829003\tvalid_1's rmse: 0.89499\n",
      "[186]\ttraining's rmse: 0.828952\tvalid_1's rmse: 0.895065\n",
      "[187]\ttraining's rmse: 0.828853\tvalid_1's rmse: 0.895119\n",
      "[188]\ttraining's rmse: 0.828798\tvalid_1's rmse: 0.895165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[189]\ttraining's rmse: 0.828712\tvalid_1's rmse: 0.895137\n",
      "[190]\ttraining's rmse: 0.828625\tvalid_1's rmse: 0.895143\n",
      "[191]\ttraining's rmse: 0.828551\tvalid_1's rmse: 0.895134\n",
      "[192]\ttraining's rmse: 0.828378\tvalid_1's rmse: 0.895434\n",
      "[193]\ttraining's rmse: 0.82832\tvalid_1's rmse: 0.89539\n",
      "[194]\ttraining's rmse: 0.828247\tvalid_1's rmse: 0.895288\n",
      "[195]\ttraining's rmse: 0.828201\tvalid_1's rmse: 0.895128\n",
      "[196]\ttraining's rmse: 0.828094\tvalid_1's rmse: 0.894886\n",
      "[197]\ttraining's rmse: 0.828059\tvalid_1's rmse: 0.89489\n",
      "[198]\ttraining's rmse: 0.827993\tvalid_1's rmse: 0.894858\n",
      "[199]\ttraining's rmse: 0.827912\tvalid_1's rmse: 0.897846\n",
      "Early stopping, best iteration is:\n",
      "[139]\ttraining's rmse: 0.833256\tvalid_1's rmse: 0.892898\n",
      "total rmse : 0.8863172649851351\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.8863  \u001b[0m | \u001b[0m 0.9376  \u001b[0m | \u001b[0m 19.38   \u001b[0m | \u001b[0m 2.076   \u001b[0m | \u001b[0m 0.9334  \u001b[0m | \u001b[0m 1.981   \u001b[0m | \u001b[0m 0.6971  \u001b[0m | \u001b[0m 510.4   \u001b[0m | \u001b[0m 2.236   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 4.935   \u001b[0m | \u001b[0m 108.7   \u001b[0m | \u001b[0m 216.1   \u001b[0m | \u001b[0m 9.946   \u001b[0m | \u001b[0m 83.01   \u001b[0m |\n",
      "[1]\ttraining's rmse: 1.0519\tvalid_1's rmse: 1.03235\n",
      "Training until validation scores don't improve for 60 rounds.\n",
      "[2]\ttraining's rmse: 0.969627\tvalid_1's rmse: 0.972193\n",
      "[3]\ttraining's rmse: 0.92563\tvalid_1's rmse: 0.942091\n",
      "[4]\ttraining's rmse: 0.893164\tvalid_1's rmse: 0.920968\n",
      "[5]\ttraining's rmse: 0.875759\tvalid_1's rmse: 0.912149\n",
      "[6]\ttraining's rmse: 0.864956\tvalid_1's rmse: 0.906251\n",
      "[7]\ttraining's rmse: 0.856805\tvalid_1's rmse: 0.904011\n",
      "[8]\ttraining's rmse: 0.851307\tvalid_1's rmse: 0.902889\n",
      "[9]\ttraining's rmse: 0.846704\tvalid_1's rmse: 0.901334\n",
      "[10]\ttraining's rmse: 0.843111\tvalid_1's rmse: 0.900142\n",
      "[11]\ttraining's rmse: 0.841007\tvalid_1's rmse: 0.899144\n",
      "[12]\ttraining's rmse: 0.838078\tvalid_1's rmse: 0.898711\n",
      "[13]\ttraining's rmse: 0.836448\tvalid_1's rmse: 0.897336\n",
      "[14]\ttraining's rmse: 0.834716\tvalid_1's rmse: 0.896624\n",
      "[15]\ttraining's rmse: 0.832848\tvalid_1's rmse: 0.89629\n",
      "[16]\ttraining's rmse: 0.830988\tvalid_1's rmse: 0.893449\n",
      "[17]\ttraining's rmse: 0.829358\tvalid_1's rmse: 0.892699\n",
      "[18]\ttraining's rmse: 0.827959\tvalid_1's rmse: 0.892537\n",
      "[19]\ttraining's rmse: 0.826686\tvalid_1's rmse: 0.892708\n",
      "[20]\ttraining's rmse: 0.825681\tvalid_1's rmse: 0.893104\n",
      "[21]\ttraining's rmse: 0.824101\tvalid_1's rmse: 0.892681\n",
      "[22]\ttraining's rmse: 0.821808\tvalid_1's rmse: 0.893684\n",
      "[23]\ttraining's rmse: 0.820078\tvalid_1's rmse: 0.888451\n",
      "[24]\ttraining's rmse: 0.819244\tvalid_1's rmse: 0.887471\n",
      "[25]\ttraining's rmse: 0.818755\tvalid_1's rmse: 0.887457\n",
      "[26]\ttraining's rmse: 0.818193\tvalid_1's rmse: 0.88742\n",
      "[27]\ttraining's rmse: 0.817364\tvalid_1's rmse: 0.887463\n",
      "[28]\ttraining's rmse: 0.816665\tvalid_1's rmse: 0.888705\n",
      "[29]\ttraining's rmse: 0.816\tvalid_1's rmse: 0.88858\n",
      "[30]\ttraining's rmse: 0.815032\tvalid_1's rmse: 0.889297\n",
      "[31]\ttraining's rmse: 0.814559\tvalid_1's rmse: 0.88951\n",
      "[32]\ttraining's rmse: 0.81378\tvalid_1's rmse: 0.889504\n",
      "[33]\ttraining's rmse: 0.813434\tvalid_1's rmse: 0.889387\n",
      "[34]\ttraining's rmse: 0.812761\tvalid_1's rmse: 0.891472\n",
      "[35]\ttraining's rmse: 0.811823\tvalid_1's rmse: 0.891394\n",
      "[36]\ttraining's rmse: 0.810751\tvalid_1's rmse: 0.891213\n",
      "[37]\ttraining's rmse: 0.810221\tvalid_1's rmse: 0.891251\n",
      "[38]\ttraining's rmse: 0.809175\tvalid_1's rmse: 0.890766\n",
      "[39]\ttraining's rmse: 0.808613\tvalid_1's rmse: 0.891407\n",
      "[40]\ttraining's rmse: 0.808128\tvalid_1's rmse: 0.891169\n",
      "[41]\ttraining's rmse: 0.80762\tvalid_1's rmse: 0.891152\n",
      "[42]\ttraining's rmse: 0.806917\tvalid_1's rmse: 0.891317\n",
      "[43]\ttraining's rmse: 0.80603\tvalid_1's rmse: 0.891834\n",
      "[44]\ttraining's rmse: 0.805407\tvalid_1's rmse: 0.891612\n",
      "[45]\ttraining's rmse: 0.805074\tvalid_1's rmse: 0.891588\n",
      "[46]\ttraining's rmse: 0.804454\tvalid_1's rmse: 0.896491\n",
      "[47]\ttraining's rmse: 0.804133\tvalid_1's rmse: 0.896447\n",
      "[48]\ttraining's rmse: 0.803715\tvalid_1's rmse: 0.896012\n",
      "[49]\ttraining's rmse: 0.803336\tvalid_1's rmse: 0.895894\n",
      "[50]\ttraining's rmse: 0.80238\tvalid_1's rmse: 0.896567\n",
      "[51]\ttraining's rmse: 0.801555\tvalid_1's rmse: 0.896591\n",
      "[52]\ttraining's rmse: 0.801248\tvalid_1's rmse: 0.896825\n",
      "[53]\ttraining's rmse: 0.800956\tvalid_1's rmse: 0.896788\n",
      "[54]\ttraining's rmse: 0.800748\tvalid_1's rmse: 0.89594\n",
      "[55]\ttraining's rmse: 0.800389\tvalid_1's rmse: 0.896047\n",
      "[56]\ttraining's rmse: 0.799803\tvalid_1's rmse: 0.895624\n",
      "[57]\ttraining's rmse: 0.799163\tvalid_1's rmse: 0.895135\n",
      "[58]\ttraining's rmse: 0.798638\tvalid_1's rmse: 0.896859\n",
      "[59]\ttraining's rmse: 0.798293\tvalid_1's rmse: 0.896708\n",
      "[60]\ttraining's rmse: 0.798077\tvalid_1's rmse: 0.896711\n",
      "[61]\ttraining's rmse: 0.797306\tvalid_1's rmse: 0.896929\n",
      "[62]\ttraining's rmse: 0.796962\tvalid_1's rmse: 0.896484\n",
      "[63]\ttraining's rmse: 0.796668\tvalid_1's rmse: 0.896311\n",
      "[64]\ttraining's rmse: 0.796186\tvalid_1's rmse: 0.896094\n",
      "[65]\ttraining's rmse: 0.795663\tvalid_1's rmse: 0.896212\n",
      "[66]\ttraining's rmse: 0.795446\tvalid_1's rmse: 0.89618\n",
      "[67]\ttraining's rmse: 0.79481\tvalid_1's rmse: 0.896312\n",
      "[68]\ttraining's rmse: 0.794503\tvalid_1's rmse: 0.8964\n",
      "[69]\ttraining's rmse: 0.79403\tvalid_1's rmse: 0.895734\n",
      "[70]\ttraining's rmse: 0.793607\tvalid_1's rmse: 0.895542\n",
      "[71]\ttraining's rmse: 0.793387\tvalid_1's rmse: 0.895426\n",
      "[72]\ttraining's rmse: 0.793143\tvalid_1's rmse: 0.895421\n",
      "[73]\ttraining's rmse: 0.792919\tvalid_1's rmse: 0.895386\n",
      "[74]\ttraining's rmse: 0.792395\tvalid_1's rmse: 0.895447\n",
      "[75]\ttraining's rmse: 0.792116\tvalid_1's rmse: 0.895433\n",
      "[76]\ttraining's rmse: 0.791762\tvalid_1's rmse: 0.895355\n",
      "[77]\ttraining's rmse: 0.791492\tvalid_1's rmse: 0.898207\n",
      "[78]\ttraining's rmse: 0.79116\tvalid_1's rmse: 0.898966\n",
      "[79]\ttraining's rmse: 0.790703\tvalid_1's rmse: 0.898898\n",
      "[80]\ttraining's rmse: 0.790407\tvalid_1's rmse: 0.898997\n",
      "[81]\ttraining's rmse: 0.790074\tvalid_1's rmse: 0.899107\n",
      "[82]\ttraining's rmse: 0.789848\tvalid_1's rmse: 0.899985\n",
      "[83]\ttraining's rmse: 0.789634\tvalid_1's rmse: 0.899946\n",
      "[84]\ttraining's rmse: 0.789347\tvalid_1's rmse: 0.899892\n",
      "[85]\ttraining's rmse: 0.788993\tvalid_1's rmse: 0.899857\n",
      "[86]\ttraining's rmse: 0.788716\tvalid_1's rmse: 0.899974\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttraining's rmse: 0.818193\tvalid_1's rmse: 0.88742\n",
      "total rmse : 0.8874197536783388\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.8874  \u001b[0m | \u001b[0m 0.949   \u001b[0m | \u001b[0m 13.2    \u001b[0m | \u001b[0m 3.533   \u001b[0m | \u001b[0m 0.9008  \u001b[0m | \u001b[0m 1.084   \u001b[0m | \u001b[0m 0.3077  \u001b[0m | \u001b[0m 33.85   \u001b[0m | \u001b[0m 7.357   \u001b[0m | \u001b[0m 111.8   \u001b[0m | \u001b[0m 9.913   \u001b[0m | \u001b[0m 196.2   \u001b[0m | \u001b[0m 26.0    \u001b[0m | \u001b[0m 2.828   \u001b[0m | \u001b[0m 23.69   \u001b[0m |\n",
      "=================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "pbounds = {\n",
    "    'num_leaves': (20, 120),\n",
    "    'max_depth': (2, 20),\n",
    "    'min_child_weight': (0.5,10.0),\n",
    "    'feature_fraction': (0.9, 1.0),\n",
    "    'bagging_fraction': (0.9, 1.0),\n",
    "    'bagging_freq': (1.0, 20.0),\n",
    "    'learning_rate': (0.001, 0.8),\n",
    "    'lambda_param': (0.0, 2.0),\n",
    "    'min_split_gain': (0.0, 10.0),\n",
    "    'min_child_samples': (20, 150),\n",
    "    'min_data_per_group': (20, 400),\n",
    "    'cat_smooth': (0.0, 5.0),\n",
    "    'min_data_in_leaf': (10, 200),\n",
    "    'max_bin': (32, 512)\n",
    "}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=bayesian_lgb,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    ")\n",
    "optimizer.maximize(\n",
    "    init_points=1,\n",
    "    n_iter=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': -0.8763972148816184, 'params': {'bagging_fraction': 0.9374540118847363, 'bagging_freq': 19.063571821788408, 'cat_smooth': 3.6599697090570253, 'feature_fraction': 0.9598658484197037, 'lambda_param': 0.31203728088487304, 'learning_rate': 0.12563962174862592, 'max_bin': 59.88013384073574, 'max_depth': 17.591170623948834, 'min_child_samples': 98.14495152661715, 'min_child_weight': 7.226689489062432, 'min_data_in_leaf': 13.911053916202466, 'min_data_per_group': 388.5657438215578, 'min_split_gain': 8.324426408004218, 'num_leaves': 41.23391106782762}}\n"
     ]
    }
   ],
   "source": [
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did not work :\n",
    "- Preprocess features with PCA and n_components=len(features)\n",
    "- Xgboost with the below hyperparameters alone:  \n",
    "model = XGBRegressor(  \n",
    "    max_depth=10,  \n",
    "    n_estimators=1000,  \n",
    "    min_child_weight=0.5,   \n",
    "    colsample_bytree=0.8,   \n",
    "    subsample=0.8,   \n",
    "    eta=0.1,  \n",
    "    seed=42)  \n",
    "- Stacking : linear regression (using Scikit learn LinearRegression() between lgb and xgb predictions\n",
    "- Deep Learning : See Deep Learning Models notebook\n",
    "- Building a specific model only for new items with no historical data\n",
    "- Rounding predictions before submission\n",
    "- Forcing predicted value below a small threshold (e.g. 0.2) to be exactly 0.0 before submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
